predictions.csv (for procedural is done on last 5 testcases from traindataset which were removed from training (we only trained on first 1000))

# üß† Math Solver ‚Äî AI-Powered Reasoning Engine  
> A hybrid Chain-of-Thought and Program-Aided solver for mathematical reasoning, built to push the limits of LLM-based problem solving.

---

## üöÄ Overview

**Math Solver** is an intelligent reasoning engine that combines **Chain-of-Thought prompting**, **Program-Aided Language (PAL)** execution, and **self-consistency verification** to solve complex math problems.  
It is designed to **mimic human problem-solving reasoning** ‚Äî breaking down problems step by step and verifying answers programmatically.  

The system supports:
- Arithmetic reasoning  
- Word problems  
- Algebraic and logical tasks  
- Custom prompt templates for fine-grained control  

This project was developed for an AI hackathon to showcase how structured prompting and reasoning verification can drastically improve LLM accuracy in math tasks.

---

## üß© Features

‚úÖ **Multi-Strategy Solvers**  
- *Chain-of-Thought (CoT)* reasoning  
- *Program-Aided Language (PAL)* solving via Python execution  
- *Self-Consistency voting* across multiple reasoning paths  

‚úÖ **Configurable Architecture**  
- Modular design via `config.yaml`  
- Plug-and-play solvers: CoT, PAL, or hybrid  

‚úÖ **Data-Driven Evaluation**  
- Evaluate models on datasets like `math_train_9k.csv` or `testmath.csv`  
- Auto-logs predictions and reasoning traces  

‚úÖ **Prompt Engineering Suite**  
- Templates for arithmetic, reasoning, and PAL prompts  
- Few-shot retriever for efficient contextualization  

‚úÖ **Explainable Outputs**  
- Generates structured reasoning traces (`traces.jsonl`)  
- Produces verified answers in `outputs/math_predictions.csv`

---

## üèóÔ∏è Project Structure

```
math-solver-full/
‚îú‚îÄ‚îÄ config.yaml                # Configuration for model + solver setup
‚îú‚îÄ‚îÄ main.py                    # Main entry point
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ PLACEHOLDER_KEYS.txt       # Add your API keys here
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ math_train_9k.csv
‚îÇ   ‚îî‚îÄ‚îÄ testmath.csv
‚îÇ
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ math_predictions.csv
‚îÇ   ‚îî‚îÄ‚îÄ traces.jsonl
‚îÇ
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îú‚îÄ‚îÄ cot_arithmetic_template.txt
‚îÇ   ‚îú‚îÄ‚îÄ cot_reasoning_template.txt
‚îÇ   ‚îî‚îÄ‚îÄ pal_template.txt
‚îÇ
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ cot_solver.py
    ‚îú‚îÄ‚îÄ pal_solver.py
    ‚îú‚îÄ‚îÄ self_consistency.py
    ‚îú‚îÄ‚îÄ verifier.py
    ‚îú‚îÄ‚îÄ normalizer.py
    ‚îú‚îÄ‚îÄ writer.py
    ‚îú‚îÄ‚îÄ loader.py
    ‚îî‚îÄ‚îÄ few_shot_retriever.py
```

---

## ‚öôÔ∏è Installation

### 1Ô∏è‚É£ Clone the repo
```bash
git clone https://github.com/yourusername/math-solver.git
cd math-solver
```

### 2Ô∏è‚É£ Install dependencies
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Add your API keys  
Create a `.env` or update `PLACEHOLDER_KEYS.txt` with your OpenAI or LLM provider keys:
```
OPENAI_API_KEY=your_key_here
```

---

## üß™ Usage

### Run the solver
```bash
python main.py --config config.yaml
```

### Choose a solving strategy  
Edit `config.yaml` to switch between:
```yaml
solver_type: "cot"     # Chain of Thought
# solver_type: "pal"   # Program-Aided Language
# solver_type: "hybrid" # CoT + PAL with self-consistency
```

### Evaluate on dataset
```bash
python main.py --eval --data data/testmath.csv
```

### Output
- `outputs/math_predictions.csv` ‚Üí final answers  
- `outputs/traces.jsonl` ‚Üí reasoning traces for analysis  

---

## üîç Example

**Input**
```
If 3x + 2 = 11, what is x?
```

**Chain-of-Thought Trace**
```
3x + 2 = 11 ‚Üí 3x = 9 ‚Üí x = 3
```

**Output**
```
Answer: 3
```

---

## üß† Architecture

```text
Question ‚Üí Retriever ‚Üí Prompt Builder ‚Üí Solver (CoT/PAL) ‚Üí Verifier ‚Üí Writer
```

- **Retriever**: Selects few-shot examples  
- **Solver**: Executes reasoning (text-based or Python-based)  
- **Verifier**: Checks and normalizes results  
- **Writer**: Stores outputs for evaluation  

---

## üìä Evaluation Metrics

| Metric | Description |
|--------|--------------|
| Accuracy | Correct predictions vs ground truth |
| Consistency | Agreement across reasoning samples |
| Trace Quality | Depth and clarity of reasoning steps |

---

## üåü Future Enhancements

- üîß Web UI for real-time problem solving  
- üß© Integration with symbolic math (SymPy)  
- ü§ñ Reinforcement learning from reasoning feedback  
- üìö Multi-domain reasoning (physics, logic, etc.)

---

## üßë‚Äçüíª Contributors

- **Team Name** ‚Äì Interstellar 
- **Team Member(s)** ‚Äì Amey Taksali, Pakhi Debnath, Ridhima Gupta 

---

## üèÅ License

This project is released under the **MIT License**.  
Feel free to fork, improve, and experiment responsibly.


# üß† Multi-Hop Reasoning Engine  
> *An intelligent retrieval-augmented reasoning framework for multi-step logical inference powered by LLMs.*

---

## üöÄ Overview

**Multi-Hop Reasoning Engine** is an advanced AI system designed to perform *multi-step reasoning* across textual information.  
Instead of answering questions in isolation, it chains evidence from multiple passages or data points ‚Äî similar to how humans connect facts to derive conclusions.

The project demonstrates a **hybrid architecture combining retrieval, semantic embeddings, and language model reasoning**, making it ideal for:
- Multi-hop question answering (QA)
- Knowledge graph reasoning  
- Contextual information synthesis  
- Explainable AI pipelines  

This system was built for an **AI Hackathon** to explore how LLMs can reason over structured and unstructured data in a transparent and traceable manner.

---

## üß© Features

‚úÖ **Multi-Hop Reasoning**  
Performs multi-step logical inference over multiple sources of information.  

‚úÖ **Retrieval-Augmented Generation (RAG)**  
Integrates semantic retrieval and context injection before reasoning.  

‚úÖ **Dynamic Prompt Construction**  
Prompts are programmatically built using templates and retrieved context.  

‚úÖ **Explainable Traces**  
Outputs include reasoning paths for interpretability and debugging.  

‚úÖ **Plug-and-Play Model Interface**  
Supports various LLM APIs via modular client wrappers.  

‚úÖ **Caching and Indexing**  
Optimized for repeated queries using cached embeddings and semantic indices.

---

## üèóÔ∏è Project Structure

```
multi-hop-reasoning/
‚îú‚îÄ‚îÄ config.py                # Global configurations
‚îú‚îÄ‚îÄ main.py                  # Entry point for running reasoning pipeline
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ .env.example             # Example environment file for API keys
‚îú‚îÄ‚îÄ train.csv                # Training data (if applicable)
‚îú‚îÄ‚îÄ test.csv                 # Test data
‚îÇ
‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îî‚îÄ‚îÄ build_index.py       # Builds embedding-based index for retrieval
‚îÇ
‚îú‚îÄ‚îÄ indexing/
‚îÇ   ‚îî‚îÄ‚îÄ semantic_index.py    # Handles semantic search and retrieval
‚îÇ
‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îú‚îÄ‚îÄ reasoning_engine.py  # Core reasoning pipeline
‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py        # LLM API integration
‚îÇ   ‚îî‚îÄ‚îÄ response_parser.py   # Cleans and parses model responses
‚îÇ
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îî‚îÄ‚îÄ prompt_builder.py    # Builds structured prompts dynamically
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py       # Loads datasets and text corpora
‚îÇ   ‚îú‚îÄ‚îÄ embedding_generator.py # Creates embeddings for retrieval
‚îÇ   ‚îî‚îÄ‚îÄ pattern_extractor.py # Extracts logical patterns from text
‚îÇ
‚îú‚îÄ‚îÄ indices/                 # Storage for built indices
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ
‚îî‚îÄ‚îÄ cache/                   # Cache for embeddings and responses
    ‚îî‚îÄ‚îÄ .gitkeep
```

---

## ‚öôÔ∏è Installation

### 1Ô∏è‚É£ Clone the repository
```bash
git clone https://github.com/yourusername/multi-hop-reasoning.git
cd multi-hop-reasoning
```

### 2Ô∏è‚É£ Set up environment
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Configure API keys  
Create a `.env` file based on `.env.example`:
```
OPENAI_API_KEY=your_api_key_here
```

---

## üß™ Usage

### Run the reasoning pipeline
```bash
python main.py --config config.py
```

### Example: Multi-hop question
**Input:**
> "Who is the author of the book written by the person who invented the light bulb?"

**Pipeline Steps:**
1. Retrieve related facts about the light bulb.  
2. Find who invented it (Thomas Edison).  
3. Search for books authored by Edison.  
4. Combine the information and reason through context.  

**Output:**
```
Answer: Thomas Edison is the author of "The Wizard of Menlo Park".
Reasoning Path: [light bulb ‚Üí Thomas Edison ‚Üí authored book]
```

---

## üîç Core Architecture

```text
Question ‚Üí Preprocessor ‚Üí Retriever ‚Üí Prompt Builder ‚Üí LLM Reasoning Engine ‚Üí Parser ‚Üí Final Answer
```

- **Preprocessor:** Tokenizes and indexes text corpora  
- **Retriever:** Retrieves top-k relevant chunks using semantic similarity  
- **Prompt Builder:** Dynamically constructs reasoning prompts  
- **Reasoning Engine:** Performs iterative multi-hop reasoning  
- **Parser:** Extracts structured answers and reasoning chains  

---

## ‚ö° Example Workflow

```bash
# Build embedding index
python preprocessing/build_index.py

# Run inference
python inference/reasoning_engine.py --question "What was discovered by the student of Socrates?"
```

**Output Example:**
```
Answer: Plato discovered the Theory of Forms.
Evidence Chain: [Socrates ‚Üí Plato ‚Üí Theory of Forms]
```

---

## üß† How It Works

### 1. Semantic Retrieval  
Uses **vector embeddings** to find relevant text chunks for a given query.

### 2. Dynamic Prompt Construction  
Builds structured multi-hop prompts with evidence chaining.

### 3. Iterative Reasoning  
Executes multi-turn reasoning using the LLM, guided by retrieved context.

### 4. Parsing and Verification  
Parses reasoning traces, verifies consistency, and formats the final output.

---

## üìä Evaluation

| Metric | Description |
|---------|--------------|
| Reasoning Accuracy | Correctness of multi-hop chains |
| Evidence Coverage | Number of relevant facts retrieved |
| Coherence | Logical flow of reasoning steps |
| Interpretability | Clarity of generated reasoning traces |

---

## üåü Future Enhancements

- üß© Integration with **Graph Databases (Neo4j)** for structured reasoning  
- üßÆ Support for **Knowledge Graph Augmentation**  
- üß† **Reinforcement Learning from Reasoning Feedback (RLRF)**  
- üåê Web dashboard for visualizing reasoning paths  

---

## üßë‚Äçüíª Contributors

- **Team Name** ‚Äì Team Interstellar
- **Team Member(s)** ‚Äì Amey Taksali, Pakhi Debnath, Ridhima Gupta 

---

## üèÅ License

This project is released under the **MIT License**.  
You are free to use, modify, and share it with attribution.
